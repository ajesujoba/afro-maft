# AfroXLMR: Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning

This repository contains code and data for our COLING22 paper [***Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning***](https://aclanthology.org/2022.coling-1.382/). 





If you find this reposity useful, please consider citing our paper.


## Citation
```
@inproceedings{alabi-etal-2022-adapting,
    title = "Adapting Pre-trained Language Models to {A}frican Languages via Multilingual Adaptive Fine-Tuning",
    author = "Alabi, Jesujoba O.  and
      Adelani, David Ifeoluwa  and
      Mosbach, Marius  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.382",
    pages = "4336--4349",
}
```
